#! /usr/bin/env python
"""
Execution script for snakemake workflows.
"""
__author__ = "Evangelos A. Dimopoulos, Evan K. Irving-Pease"
__copyright__ = "Copyright 2020, University of Oxford"
__email__ = "antonisdim41@gmail.com"
__license__ = "MIT"

import argparse
import os.path
import sys

import snakemake
import yaml

from multiprocessing import cpu_count
from psutil import virtual_memory

MEGABYTE = float(1024 ** 2)
MAX_MEM_MB = virtual_memory().total / MEGABYTE

thisdir = os.path.abspath(os.path.dirname(__file__))

# maximum concurrent Entrez requests
MAX_ENTREZ_REQUESTS = 3


# TODO come up with solution so we don't need to activate the conda environment if we split the environment file into
#  smaller ones and assign them to the individual rules, then we don't need to pre-build the environment
#  https://snakemake.readthedocs.io/en/stable/tutorial/additional_features.html#automatic-deployment-of-software
#  -dependencies


def main(args):
    # first, find the Snakefile
    snakefile = os.path.join(thisdir, "Snakefile")
    if not os.path.exists(snakefile):
        sys.stderr.write("Error: cannot find Snakefile at {}\n".format(snakefile))
        sys.exit(-1)

    # next, find the workflow config file

    if os.path.exists(args.config_yaml):
        with open(args.config_yaml) as fin:
            config_yaml = yaml.safe_load(fin)
    else:
        config_yaml = {}
        print("no config provided")

    config_args = vars(args)

    # config = {**config_yaml, **config_args}
    config_yaml.update((k, v) for k, v in config_args.items() if v is not None)
    config = config_yaml

    # Targets

    if args.input_mode == "COLLAPSED":
        config["PE_ANCIENT"] = True
        config["PE_MODERN"] = False
        config["SE"] = False
    elif args.input_mode == "PE":
        config["PE_ANCIENT"] = False
        config["PE_MODERN"] = True
        config["SE"] = False
    elif args.input_mode == "SE":
        config["PE_ANCIENT"] = False
        config["PE_MODERN"] = False
        config["SE"] = True
    else:
        raise RuntimeError(
            "You have provided an invalid option as input mode. "
            "Please provide only one of the following COLLAPSED, PE or SE."
        )

    if args.custom_seq_file != "":
        config["with_custom_sequences"] = True
    else:
        config["with_custom_sequences"] = False

    if args.custom_acc_file != "":
        config["with_custom_accessions"] = True
    else:
        config["with_custom_accessions"] = False

    target_list = []

    if config["trim_adapters"] or args.data_preprocess:
        input_mode = ""
        if config["PE_ANCIENT"]:
            input_mode = "PE_anc"
        elif config["PE_MODERN"]:
            input_mode = "PE_mod"
        elif config["SE"]:
            input_mode = "SE"

        data_preprocessing = ""
        if config["PE_MODERN"]:
            data_preprocessing = "fastq_inputs/{input_mode}/{sample}_R1_adRm.fastq.gz".format(
                sample=config["sample_name"], input_mode=input_mode
            )
        elif config["PE_ANCIENT"] or config["SE"]:
            data_preprocessing = "fastq_inputs/{input_mode}/{sample}_adRm.fastq.gz".format(
                sample=config["sample_name"], input_mode=input_mode
            )
        target_list.append(data_preprocessing)

    if args.build_db:
        if config["with_entrez_query"]:
            target_list.append(
                "{query}/bowtie/{query}_entrez.fasta.gz".format(
                    query=config["query_name"]
                )
            )
        if config["with_refseq_rep"]:
            target_list.append(
                "{query}/bowtie/refseq_rep_refseq_prok.fasta.gz".format(
                    query=config["query_name"]
                )
            )
        if config["with_custom_sequences"]:
            target_list.append(
                "{query}/bowtie/{query}_custom_seqs.fasta.gz".format(
                    query=config["query_name"]
                )
            )
        if config["with_custom_accessions"]:
            target_list.append(
                "{query}/bowtie/{query}_custom_acc.fasta.gz".format(
                    query=config["query_name"]
                )
            )

    if args.index_db:
        target_list.append(
            "database/idx_database_{query}.done".format(query=config["query_name"])
        )

    if args.aln_filter_index:
        target_list.append(
            "{query}/bowtie/bowtie_index.done".format(query=config["query_name"])
        )

    if args.complete_db_creation:
        target_list.append(
            "database/idx_database_{query}.done".format(query=config["query_name"])
        )
        target_list.append(
            "{query}/bowtie/bowtie_index.done".format(query=config["query_name"])
        )

    if args.aln_filter:
        bowtie = ""
        if config["PE_MODERN"]:
            bowtie = "{query}/fastq/PE/{sample}_mapq_pair.readlen".format(
                query=config["query_name"], sample=config["sample_name"]
            )
        elif config["PE_ANCIENT"] or config["SE"]:
            bowtie = "{query}/fastq/SE/{sample}_mapq.readlen".format(
                query=config["query_name"], sample=config["sample_name"]
            )
        target_list.append(bowtie)

    if args.aln_meta:
        target_list.append(
            "{query}/sigma/{sample}_alignments.done".format(
                query=config["query_name"], sample=config["sample_name"]
            )
        )

    if args.probabilities:
        target_list.append(
            "{query}/probabilities/{sample}/{sample}_posterior_probabilities.csv".format(
                query=config["query_name"], sample=config["sample_name"]
            )
        )

    if args.abundances:
        target_list.append(
            "{query}/probabilities/{sample}/{sample}_posterior_abundance.tsv".format(
                query=config["query_name"], sample=config["sample_name"]
            )
        )

    if args.mapdamage:
        target_list.append(
            "{query}/mapdamage/{sample}_mapdamage.done".format(
                query=config["query_name"], sample=config["sample_name"]
            )
        )

    if args.sra_lookup:
        if config["PE_MODERN"]:
            config[
                "fastq_R1"
            ] = "fastq_inputs/PE_mod/{accession}_R1_adRm.fastq.gz".format(
                accession=config["sample_name"]
            )
            config[
                "fastq_R2"
            ] = "fastq_inputs/PE_mod/{accession}_R2_adRm.fastq.gz".format(
                accession=config["sample_name"]
            )
        elif config["PE_ANCIENT"] or config["SE"]:
            config["fastq"] = "fastq_inputs/SE/{accession}_adRm.fastq.gz".format(
                accession=config["sample_name"]
            )

    # Print the details for the run params

    defaults = {
        "dry-run": False,
        "entrez_batchsize": 5,
        "entrez_rank": "species",
        "mismatch_probability": 0.05,
        "bowtie2_treads": 1,
        "sra_lookup": False,
        "with_refseq_rep": False,
        "with_entrez_query": True,
        "trim_adapters": False,
        "mem_rescale_factor": 2.5,
        "data_preprocess": False,
        "build_db": False,
        "index_db": False,
        "aln_filter_index": False,
        "aln_filter": False,
        "aln_meta": False,
        "probabilities": False,
        "abundances": False,
        "mapdamage": False,
        "complete_db_creation": False,
        "custom_acc_file": "",
        "custom_seq_file": "",
        "touch": False,
        "cores": cpu_count(),
        "no_lock": False,
        "unlock": False,
        "debug": False,
    }

    user_options = {k: v for k, v in config.items() if (k, v) not in defaults.items()}
    user = {
        k: v
        for k, v in user_options.items()
        if k
        not in [
            "PE_ANCIENT",
            "PE_MODERN",
            "SE",
            "with_custom_sequences",
            "with_custom_accessions",
        ]
    }

    print("--------")
    print("RUN DETAILS")
    print("\n\tSnakefile: {}".format(snakefile))
    print("\n\tConfig Parameters:\n")
    if args.debug:
        for (key, value,) in config.items():
            print(f"{key:35}{value}")
    else:
        for (key, value,) in user.items():
            print(f"{key:35}{value}")

    print("\n\tTarget Output Files:\n")
    for target in target_list:
        print(target)
    print("--------")

    if args.debug:
        printshellcmds = True
        keepgoing = False
    else:
        printshellcmds = False
        keepgoing = True

    status = snakemake.snakemake(
        snakefile,
        config=config,
        targets=target_list,
        printshellcmds=printshellcmds,
        dryrun=args.dry_run,
        cores=int(args.cores),
        keepgoing=keepgoing,
        restart_times=3,  # TODO find a better solution to this... 15 is way too many!
        touch=args.touch,
        forceall=args.touch,
        lock=args.no_lock,
        unlock=args.unlock,
        show_failed_logs=args.debug,
        resources={"entrez_api": MAX_ENTREZ_REQUESTS},
    )

    # translate "success" into shell exit code of 0
    return 0 if status else 1


if __name__ == "__main__":

    # TODO as there are now multiple modes for running RIP, then you should refactor the CLI options such that they are
    #   run as proper subcommands, e.g. https://chase-seibert.github.io/blog/2014/03/21/python-multilevel-argparse.html
    #   we should not be asking users if their samples are PE or SE if all they want to do it build the database

    parser = argparse.ArgumentParser(
        description="run snakemake workflows",
        usage="""run <config.yaml>
    Run RIP workflows, using the given workflow name & parameters file.
    """,
    )
    #
    # subparsers = parser.add_subparsers(help='sub-command help', title="actions")
    # parser = subparsers.add_parser('db_creation', help='DB creation help message', add_help=False, parents=[parser])
    # parser = subparsers.add_parser('sample_analysis', help='Sample analysis help message', add_help=False,
    #     parents=[parser])

    parser.add_argument("--config_yaml", default="")
    parser.add_argument("--dry-run", action="store_true")
    parser.add_argument(
        "--query_name",
        help="Name of the query and the output directory. Mandatory <str>",
        metavar="",
    )
    parser.add_argument(
        "--entrez_query",
        help="Actual NCBI query in the NCBI query language",
        metavar="",
    )
    parser.add_argument(
        "--entrez_email",
        help="Email address for NCBI identification. Mandatory.",
        metavar="",
    )
    parser.add_argument(
        "--entrez_batchsize",
        help="Batchsize for fetching records from NCBI  <int> (default: 5)",
        default=5,
        metavar="",
    )
    parser.add_argument(
        "--entrez_rank",
        help="Taxonomic rank to perform the identifications on (genus, species, subspecies, serotype) "
        "<str> (default: species)",
        default="species",
        metavar="",
    )
    parser.add_argument("--sample_name", help="Sample name prefix", metavar="")
    parser.add_argument(
        "--fastq",
        help="Path to the fastq input file. Can be raw or with adapters " "removed",
        metavar="",
    )
    parser.add_argument(
        "--fastq_R1",
        help="Path to the mate 1 fastq input file, if reads are PE. "
        "Can be raw or with adapters removed",
        metavar="",
    )
    parser.add_argument(
        "--fastq_R2",
        help="Path to the mate 2 fastq input file, if reads are PE. "
        "Can be raw or with adapters removed",
        metavar="",
    )
    parser.add_argument(
        "--mismatch_probability",
        help="Base mismatch probability <float> (default: 0.05)",
        metavar="",
        default=0.05,
    )
    parser.add_argument(
        "--bowtie2_treads",
        help="Threads for the bowtie2 alignments <int> (default: 1)",
        default=1,
        metavar="",
    )
    parser.add_argument(
        "--sra_lookup",
        help="Fetch raw data files from the SRA <bool> (default: False)",
        default=False,
        metavar="",
    )

    parser.add_argument(
        "--input_mode",
        help="Treat the data as paired end (PE), paired end collapsed (COLLAPSED) oe single end (SE) "
        "reads. Mandatory. Default mode is collapsed reads.",
        default="COLLAPSED",
        metavar="",
    )

    parser.add_argument(
        "--with_refseq_rep",
        help="Use the prokaryotic representative species of the RefSeq DB "
        "for the species id pipeline. only species no strains. "
        "either or both of with_refseq_rep and "
        "with_entrez_query should be set (default: False)",
        default=False,
    )
    parser.add_argument(
        "--with_entrez_query",
        help="Use the recordset returned from a specific entrez query in "
        "the species id pipeline. written in the NCBI query language "
        "(copy it from the website).either or both of with_refseq_rep and "
        "with_entrez_query should be set (default: True)",
        default=True,
        metavar="",
    )
    parser.add_argument(
        "--trim_adapters",
        help="Remove adapters from raw fastq files",
        default=False,
        metavar="",
    )

    parser.add_argument(
        "--custom_seq_file",
        help="TAB DELIMITED input file containing the the name of the taxon with no special characters, "
        "and an underscore '_' instead of spaces, a user defined accession code and the path of the fasta file. "
        "The fasta file that the path point to can be either uncompressed or compressed with gzip/bgzip",
        metavar="",
        default="",
    )

    parser.add_argument(
        "--custom_acc_file",
        help="TAB DELIMITED input file containing the the name of the taxon with no special characters, "
        "and an underscore '_' instead of spaces, a user defined valid NCBI nucleotide, assembly or WGS "
        "accession code. ",
        metavar="",
        default="",
    )

    parser.add_argument(
        "--specific_genera",
        nargs="+",
        help="List containing the names of specific genera "
        "the abundances should be calculated "
        "on, separated by a space character <genus1 genus2 genus3 ...>",
        metavar="",
        default=[],
    )
    parser.add_argument(
        "--mem_mb",
        help="Max memory resources allowed to be used ofr indexing the input for "
        "the filtering alignment "
        "(default: max available memory {})".format(MAX_MEM_MB),
        default=MAX_MEM_MB,
        metavar="",
    )
    parser.add_argument(
        "--mem_rescale_factor",
        help="Factor to rescale/chunk the input file for the mutlifasta "
        "index for the filtering alignment (default: 2.5)",
        metavar="",
        default=2.5,
    )
    parser.add_argument(
        "--data_preprocess",
        action="store_true",
        help="Only process raw fastq files, "
        "including downloading fastq files from the SRA, "
        "compressing them and removing any sequencing "
        "adapters  <bool> (default: False)",
    )
    parser.add_argument(
        "--build_db",
        action="store_true",
        help="Only build the database from refseq and/or "
        "other sequences retrieved from an NCBI query",
    )
    parser.add_argument(
        "--index_db",
        action="store_true",
        help="Index each taxon's reference genome that is "
        "contained in the reference database, using bowtie2 "
        " <bool> (default: False)",
    )
    parser.add_argument(
        "--aln_filter_index",
        action="store_true",
        help="Build the index required for the filtering "
        "alignment step, out of all the sequences "
        "contained in the reference database "
        " <bool> (default: False)",
    )
    parser.add_argument(
        "--aln_filter",
        action="store_true",
        help="Perform the filtering alignment step " " <bool> (default: False)",
    )
    parser.add_argument(
        "--aln_meta",
        action="store_true",
        help="Perform metagenomic alignments. "
        "The aligned reads from the filtering alignment step "
        "are aligned to each reference genomes separately "
        "using bowtie2  <bool> (default: False)",
    )
    parser.add_argument(
        "--probabilities",
        action="store_true",
        help="Taxon assignment posterior probabilities are "
        "calculated for a given sample  <bool> "
        "(default: False)",
    )
    parser.add_argument(
        "--abundances",
        action="store_true",
        help="Mean posterior abundances for each taxon in our "
        "ref db, for a given sample, are calculated "
        " <bool> (default: False)",
    )
    parser.add_argument(
        "--mapdamage",
        action="store_true",
        help="Performs a chemical damage analysis, "
        "by using mapDamage  <bool> (default: False)",
    )

    parser.add_argument(
        "--complete_db_creation",
        action="store_true",
        help="Builds the local database, indexes all the genomes individually, and prepares the filtering alignment"
        "bowtie2 index <bool> (default: False)",
    )

    parser.add_argument(
        "--touch",
        action="store_true",
        help="Touches the timestamps of the requested target files "
        "AND ALL of their dependencies  <bool> (default: False)",
    )
    parser.add_argument(
        "--cores",
        help="Number of cores for RIP to use",
        metavar="",
        default=cpu_count(),
    )
    parser.add_argument(
        "--no_lock",
        action="store_true",
        help="Apply no lock to the working directory " " <bool> (default: False)",
    )
    parser.add_argument(
        "--unlock",
        action="store_true",
        help="Unlock the working directory after smk is "
        "abruptly killed  <bool> (default: False)",
    )

    parser.add_argument(
        "--debug",
        action="store_true",
        help="Debug the RIP workflow <bool> (default: False)",
    )

    if len(sys.argv) == 1 or len(sys.argv) == 2:
        parser.print_help()
        parser.exit()

    args = parser.parse_args()

    sys.exit(main(args))
